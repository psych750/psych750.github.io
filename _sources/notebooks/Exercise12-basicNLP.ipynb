{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<sidebar>\n",
    "Psych711\n",
    "\n",
    "</sidebar>\n",
    "### Fun with strings\n",
    "\n",
    "Inside the Commons\\\\exercise\\_9 folder is a corpus of tv-show subtitles\n",
    "from a bunch of US and UK TV shows. Please write code to answer the\n",
    "following questions.\n",
    "\n",
    "1.  How many unique words are there in the US corpus? In the UK corpus?\n",
    "    Hint, you probably want to concatenate all the files from each\n",
    "    corpus into a single file so that you\\'re working with two files,\n",
    "    not 100s. See [here to refresh your command line\n",
    "    skills](http://sapir.psych.wisc.edu/wiki/index.php/Linux_Tips).\n",
    "2.  List all the words that are identical but have a longer spelling in\n",
    "    the UK vs the US corpus (e.g., color/colour, behavior/behaviour,\n",
    "    acknowledgment/acknowledgement). Now list all the words that have a\n",
    "    longer spelling in US compared to UK english.\n",
    "3.  What is the average word length in the US corpus? In the UK corpus?\n",
    "4.  What are the 10 words that are relatively more frequent in the UK\n",
    "    compared to the US texts? What are the 10 words that are relatively\n",
    "    more frequent in the US compared to the UK texts?\n",
    "    1.  The definition of relative frequency is:\n",
    "        `frequency(Word)/number_of_total_words_in_the_corpus` (*Earlier\n",
    "        version said number of unique words* This is wrong). To get the\n",
    "        words that are relatively more frequent in US than the UK\n",
    "        corpus, you\\'ll want to subtract the two frequencies and output\n",
    "        the 10 words with the largest differences. You\\'ll want to\n",
    "        ignore any words that only occur in one of the corpora (e.g.,\n",
    "        there\\'s \\'colouring\\' in the UK but not the US corpus, so it\n",
    "        wouldn\\'t make sense to compare its relative frequency).\n",
    "5.  Let\\'s now redo the analysis above, but let\\'s include only\n",
    "    relatively more frequent words \\-- those with frequencies of 40 or\n",
    "    more. That is, do the analysis above only on words that occur with a\n",
    "    frequency of \\>=40 in both the US and the UK corpus.\n",
    "6.  **Bonus**: Redo the analysis above for types instead of tokens. This\n",
    "    means that go/went/going would all count as frequencies for \\'go\\',\n",
    "    cat/cats/cat\\'s would all count as frequencies for \\'cat\\'. You can\n",
    "    use the tokenizing functions from the NLTK library OR get the\n",
    "    token-to-type conversion in the ANC-written-count\\_over9.txt file\n",
    "    inside common\\\\demos\\\\regex. If you use the ANC file to do the\n",
    "    token-to-type conversion ignore words that occur in the subtitles\n",
    "    files but not in the ANC file.\n",
    "\n",
    "All of these questions have objectively correct answers, but there are a\n",
    "variety of ways to get to them. You are free to use simple string\n",
    "matching, regular expressions, the NLTK library, and any standard Python\n",
    "libraries you think would be handy. For example, the\n",
    "[`Counter`](https://docs.python.org/2/library/collections.html#counter-objects)\n",
    "class from the `collections` library may be pretty handy, as is the\n",
    "[`difflib`](https://docs.python.org/2/library/difflib.html) library I\n",
    "mentioned in class. NLTK has functions that will allow you to do this\n",
    "exercise using just a few lines of code.\n",
    "\n",
    "[Category:Exercise](Category:Exercise \"wikilink\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
